{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"gpuClass":"standard","kaggle":{"accelerator":"none","dataSources":[{"sourceId":9142574,"sourceType":"datasetVersion","datasetId":5521933},{"sourceId":95789841,"sourceType":"kernelVersion"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/marekbajdk/employee-churn-hr?scriptVersionId=196084420\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Salifort Motors - Predicting Employee Turnover for HR department","metadata":{"id":"ysS5rgTMWpwL"}},{"cell_type":"markdown","source":"![turnover](https://whatfix.com/blog/wp-content/uploads/2022/09/employee-churn.png)","metadata":{}},{"cell_type":"markdown","source":"# **Pace: Plan Stage**\n","metadata":{"id":"psz51YkZVwtN"}},{"cell_type":"markdown","source":"### Company  and the business scenario\n\nSalifort Motors, a fictional French-based manufacturer of alternative energy vehicles, is experiencing a high rate of employee turnover. Concerned about the reasons behind this, the senior leadership team is eager to understand why many employees are leaving, whether they are resigning or being let go. The high turnover rate is financially burdensome, as the process of hiring, interviewing, and training new employees is both time-consuming and expensive.\n\nSalifort aims to cultivate a corporate culture that supports employee success and professional development while enhancing employee satisfaction and retention. If Salifort could predict which employees are likely to leave and identify the factors contributing to their departure, they could address the issue more effectively.\n\nThe HR department at Salifort Motors is keen to improve employee satisfaction and has collected data from employees. They have tasked the data analytics team with providing data-driven insights. Their key question is: what factors are likely to cause employees to leave the company?","metadata":{"id":"gLEEr6peWcF7"}},{"cell_type":"markdown","source":"###  About the dataset\n\nThe dataset **\"HR_capstone_dataset.csv\"** contains 15,000 rows and 10 columns for the variables listed below. \n\nThe original data source reference:\nhttps://www.kaggle.com/datasets/mfaisalqureshi/hr-analytics-and-job-prediction?select=HR_comma_sep.csv\n\nVariable  |Description |\n-----|-----|\nsatisfaction_level|Employee-reported job satisfaction level [0&ndash;1]|\nlast_evaluation|Score of employee's last performance review [0&ndash;1]|\nnumber_project|Number of projects employee contributes to|\naverage_monthly_hours|Average number of hours employee worked per month|\ntime_spend_company|How long the employee has been with the company (years)\nWork_accident|Whether or not the employee experienced an accident while at work\nleft|Whether or not the employee left the company\npromotion_last_5years|Whether or not the employee was promoted in the last 5 years\nDepartment|The employee's department\nsalary|The employee's salary (U.S. dollars)","metadata":{"id":"lnRdR6eacUkK"}},{"cell_type":"markdown","source":"**Stakeholders**\n\n- HR department\n- Senior leadership team","metadata":{}},{"cell_type":"markdown","source":"### Import packages","metadata":{"id":"51UAXIOLC_8P"}},{"cell_type":"code","source":"# Import packages\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# For data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# For displaying all of the columns in dataframes\npd.set_option('display.max_columns', None)\n\n# For data modeling\nfrom xgboost import XGBClassifier\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# For metrics and helpful functions\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score,\\\nf1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.tree import plot_tree\n\n# For saving models\nimport pickle","metadata":{"id":"hVWGpX9As4e1","execution":{"iopub.status.busy":"2024-09-03T17:17:57.172011Z","iopub.execute_input":"2024-09-03T17:17:57.172403Z","iopub.status.idle":"2024-09-03T17:18:00.940737Z","shell.execute_reply.started":"2024-09-03T17:17:57.172373Z","shell.execute_reply":"2024-09-03T17:18:00.939339Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load dataset","metadata":{"id":"zM2P9yLWDIjN"}},{"cell_type":"code","source":"# Load dataset into a DataFrame\ndf0 = pd.read_csv(\"/kaggle/input/employee-churn/HR_comma_sep.csv\") \n\n# Display the first few rows of the dataset\ndf0.head()","metadata":{"id":"Bs0cJR5BDPgQ","execution":{"iopub.status.busy":"2024-09-03T17:18:00.951961Z","iopub.execute_input":"2024-09-03T17:18:00.952321Z","iopub.status.idle":"2024-09-03T17:18:01.025496Z","shell.execute_reply.started":"2024-09-03T17:18:00.952292Z","shell.execute_reply":"2024-09-03T17:18:01.024081Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## EDA -  data exploration and cleaning\n\n- Understand the variables\n- Clean the dataset (missing data, redundant data, outliers)\n\n","metadata":{"id":"wF_LLorPs5G_"}},{"cell_type":"markdown","source":"### Gather basic information about the data","metadata":{"id":"3LF6h1v9FYz2"}},{"cell_type":"code","source":"# Basic information about the data\n\ndf0.info()","metadata":{"id":"6XbfdPoKurMf","scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T17:18:01.026907Z","iopub.execute_input":"2024-09-03T17:18:01.027271Z","iopub.status.idle":"2024-09-03T17:18:01.057169Z","shell.execute_reply.started":"2024-09-03T17:18:01.027241Z","shell.execute_reply":"2024-09-03T17:18:01.056006Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Gather descriptive statistics about the data","metadata":{"id":"6JMl_rQ1Fgte"}},{"cell_type":"code","source":"# Gather descriptive statistics about the data\ndf0.describe().T","metadata":{"id":"_5VRL-kzE8y1","scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T17:18:01.058747Z","iopub.execute_input":"2024-09-03T17:18:01.059124Z","iopub.status.idle":"2024-09-03T17:18:01.102586Z","shell.execute_reply.started":"2024-09-03T17:18:01.059093Z","shell.execute_reply":"2024-09-03T17:18:01.101185Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" - number of projects goes between  minimum 2 and maximum 7 and mean 3.5.\n - average monthly hours worked are in the range of 96-310 hrs with mean 201 hrs.\n - time spend in company goes between minimum 2 and maximum 10 years.","metadata":{}},{"cell_type":"code","source":"# For categorical features\nprint('Departments: \\n', df0['Department'].describe(), \n      '\\nDepartments names: \\n',\n      ', '.join(df0['Department'].unique()))\n\n\n# For categorical features\nprint('\\nSalary: \\n', df0['salary'].describe(), \n      '\\nSalary categories: \\n',\n      # Joins the unique salary values, converted to strings, with a comma and a space.\n      ', '.join(map(str, df0['salary'].unique())))","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:01.10417Z","iopub.execute_input":"2024-09-03T17:18:01.104626Z","iopub.status.idle":"2024-09-03T17:18:01.122466Z","shell.execute_reply.started":"2024-09-03T17:18:01.104586Z","shell.execute_reply":"2024-09-03T17:18:01.12115Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Rename columns","metadata":{"id":"QR7eFNU0FklJ"}},{"cell_type":"markdown","source":"As part of the data cleaning process, rename the columns as necessary. Ensure that all column names are standardized to 'snake_case', correct any misspellings, and make the column names more concise where appropriate.","metadata":{"id":"_TtE0JLPJyLF"}},{"cell_type":"code","source":"# Rename columns\ndf0 = df0.rename(columns={'average_montly_hours':'average_monthly_hours',\n                          'time_spend_company':'tenure',\n                          'project_contribution':'number_project',\n                         'Work_accident': 'work_accident',\n                         'Department':'department'})\n# Display the modified column names\ndf0.columns","metadata":{"id":"npUQA8jMFJQD","execution":{"iopub.status.busy":"2024-09-03T17:18:01.124113Z","iopub.execute_input":"2024-09-03T17:18:01.12443Z","iopub.status.idle":"2024-09-03T17:18:01.136774Z","shell.execute_reply.started":"2024-09-03T17:18:01.124405Z","shell.execute_reply":"2024-09-03T17:18:01.135276Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Check for any missing values in the data.","metadata":{"id":"GeiUUqeaBt-I"}},{"cell_type":"code","source":"# Check for missing values\ndf0.isnull().sum()","metadata":{"id":"EN9MvN0GByVV","execution":{"iopub.status.busy":"2024-09-03T17:18:01.138354Z","iopub.execute_input":"2024-09-03T17:18:01.138732Z","iopub.status.idle":"2024-09-03T17:18:01.150289Z","shell.execute_reply.started":"2024-09-03T17:18:01.138702Z","shell.execute_reply":"2024-09-03T17:18:01.149037Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we can check for any duplicate entries in the data.","metadata":{"id":"Q7ystBsdsGaL"}},{"cell_type":"code","source":"# Check for duplicates\ndf0.duplicated().sum()","metadata":{"id":"CFFLc5AOZ7-x","scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T17:18:01.155311Z","iopub.execute_input":"2024-09-03T17:18:01.15589Z","iopub.status.idle":"2024-09-03T17:18:01.17168Z","shell.execute_reply.started":"2024-09-03T17:18:01.155855Z","shell.execute_reply":"2024-09-03T17:18:01.170425Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" - We observe no missing values, however there are 3008 duplicated rows.","metadata":{}},{"cell_type":"code","source":"# Rows inspection   \ndf0[df0.duplicated()].head(10)","metadata":{"id":"ZHGlDbKAcBLM","execution":{"iopub.status.busy":"2024-09-03T17:18:01.173094Z","iopub.execute_input":"2024-09-03T17:18:01.173421Z","iopub.status.idle":"2024-09-03T17:18:01.199151Z","shell.execute_reply.started":"2024-09-03T17:18:01.173395Z","shell.execute_reply":"2024-09-03T17:18:01.19799Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dropping duplicates that could affect our model and save resulting DataFrame in a new variable as needed\ndf1 = df0.drop_duplicates(keep='first')\ndf1.head()","metadata":{"id":"wCr34Rppdjay","execution":{"iopub.status.busy":"2024-09-03T17:18:01.200575Z","iopub.execute_input":"2024-09-03T17:18:01.20101Z","iopub.status.idle":"2024-09-03T17:18:01.224981Z","shell.execute_reply.started":"2024-09-03T17:18:01.200968Z","shell.execute_reply":"2024-09-03T17:18:01.223574Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Check outliers","metadata":{"id":"4knHIoTIFu83"}},{"cell_type":"markdown","source":"Check for outliers in the data.","metadata":{"id":"EaVx2fk8GC_m"}},{"cell_type":"code","source":"# Data for plotting\nplot_data = {\n    'tenure': \"Boxplot of tenure\",\n    'average_monthly_hours': \"Boxplot of average_monthly_hours\",\n    'number_project': \"Boxplot of number_project\",\n    'satisfaction_level': \"Boxplot of satisfaction_level\",\n    'last_evaluation': \"Boxplot of last_evaluation\"\n}\n\n# Create subplots\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# Flatten 2D array of axes to 1D to simplify the process of iterating over each subplot with single loop.\naxes_flat = axes.flatten()\n\n# Use the loop to iterate over each axis and the items in plot_data,\n# creating a boxplot for each column and setting the corresponding title.\n\n# 'ax' represents current subplot axis.\n# (col_name, title) unpacked tuple from dictionary items.\nfor ax, (col_name, title) in zip(axes_flat, plot_data.items()):\n    sns.boxplot(x=df1[col_name], ax=ax, color='turquoise')\n    ax.set_title(title)\n\n# Turn off the last unused subplot\naxes_flat[-1].axis(\"off\")\n\n# Adjust layout and display\nplt.tight_layout()\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:01.226471Z","iopub.execute_input":"2024-09-03T17:18:01.226842Z","iopub.status.idle":"2024-09-03T17:18:02.134899Z","shell.execute_reply.started":"2024-09-03T17:18:01.226812Z","shell.execute_reply":"2024-09-03T17:18:02.133235Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There appear to be outliers in the 'tenure' field. We should investigate how many rows in that field contain outliers. It's important to conduct this procedure because outliers can significantly impact the performance of ML models, causing models to be overly sensitive to individual data points, leading to poor generalization on unseen data. Outliers can distort the overall distribution, affect measures of central tendency (mean, median), and influence variance estimation. Additionally, they might violate assumptions of normally distributed data, which can lead to biased estimates or incorrect conclusions if not addressed properly.","metadata":{}},{"cell_type":"code","source":"## The number of rows containing outliers\n\n# Calc of 25th & 75th percentile\nq1 = df1['tenure'].quantile(0.25)\nq3 = df1['tenure'].quantile(0.75)\n# Calc of Interquartile range\niqr = q3 - q1\n\n# Calc of lower & upper bounds\nlower_bounds = q1 - 1.5 * iqr\nupper_bounds = q3 + 1.5 * iqr\nprint('Lower limit:', lower_bounds)\nprint('Upper limit:', upper_bounds)\n\n# Identify outliers\noutliers = df1[(df1['tenure'] > upper_bounds) | (df1['tenure'] < lower_bounds)]\nnum_outliers = len(outliers)\ntotal_rows = len(df1['tenure'])\n\nprint('Number of rows containing outliers:', len(outliers))\n\n# Calculate the percentage of outliers\npercentage_outliers = (num_outliers / total_rows) * 100\nprint('Percentage of outliers in the tenure column:', round(percentage_outliers,2),\"%\")","metadata":{"id":"ohctgiHyFykI","scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T17:18:02.137157Z","iopub.execute_input":"2024-09-03T17:18:02.137652Z","iopub.status.idle":"2024-09-03T17:18:02.154614Z","shell.execute_reply.started":"2024-09-03T17:18:02.137608Z","shell.execute_reply":"2024-09-03T17:18:02.153191Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Result of EDA**\n\n- We obrserve no missing value & 3008 duplicated rows.\n- We found 824 records containing outliers that makes it 6.87% of the total count of 'tenure'.\n- Employees with less than 1.5 years or more than 5.5 years of tenure are considered outliers in this case.","metadata":{}},{"cell_type":"markdown","source":"Certain types of models are more sensitive to outliers than others. When you reach the stage of building your model, consider whether to remove outliers based on the type of model you choose to use.","metadata":{"id":"eP_rPN31Kmx0"}},{"cell_type":"code","source":"df1['tenure'].value_counts().sum()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:02.156594Z","iopub.execute_input":"2024-09-03T17:18:02.157965Z","iopub.status.idle":"2024-09-03T17:18:02.171565Z","shell.execute_reply.started":"2024-09-03T17:18:02.157902Z","shell.execute_reply":"2024-09-03T17:18:02.170031Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# pAce: Analyze Stage\n\n\n","metadata":{"id":"mA7Mz_SnI8km"}},{"cell_type":"markdown","source":"We can start off with the understanding of how many employees left and what percentage of all employees this figure represents.","metadata":{"id":"KDcWrk57kao2"}},{"cell_type":"code","source":"# Numbers of people who left vs. stayed\nprint(df1['left'].value_counts())\nprint()\n# Percentages of people who left vs. stayed\nprint(round(df1['left'].value_counts(normalize=True)*100, 2))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T17:18:02.173356Z","iopub.execute_input":"2024-09-03T17:18:02.173888Z","iopub.status.idle":"2024-09-03T17:18:02.18586Z","shell.execute_reply.started":"2024-09-03T17:18:02.173852Z","shell.execute_reply":"2024-09-03T17:18:02.184302Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data visualizations","metadata":{"id":"DmVMzXPSuYk1"}},{"cell_type":"markdown","source":"Now, investigate the variables that interest us and generate plots to visualize the relationships between them in the data.\n\nLet's create a stacked boxplot showing average_monthly_hours distributions for number_project, comparing the distributions of employees who stayed versus those who left.","metadata":{}},{"cell_type":"code","source":"# Figure and axes\nfig, ax = plt.subplots(1, 2, figsize = (22,8))\n\n# Boxplot showing `average_monthly_hours` distributions for `number_project`, \n# comparing employees who stayed versus those who left\nsns.boxplot(data=df1, x='average_monthly_hours', y='number_project', hue='left', orient=\"h\", ax=ax[0])\nax[0].invert_yaxis()\nax[0].set_title('Monthly hours by number of projects', fontsize='14')\n\n# Histogram showing distribution of `number_project`, comparing employees who stayed versus\n# those who left\nsns.histplot(data=df1, x='number_project', hue='left', multiple='dodge', shrink=2, ax=ax[1])\nax[1].set_title('Number of projects histogram', fontsize='14')\n\nplt.show()\n","metadata":{"id":"Qf0VbjX8-DBQ","execution":{"iopub.status.busy":"2024-09-03T17:18:02.187468Z","iopub.execute_input":"2024-09-03T17:18:02.187805Z","iopub.status.idle":"2024-09-03T17:18:03.155317Z","shell.execute_reply.started":"2024-09-03T17:18:02.187777Z","shell.execute_reply":"2024-09-03T17:18:03.15414Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Employees who left the organization fell into two main categories:\n  - Some worked significantly fewer hours compared to peers with similar project involvement.\n  - Others worked considerably more, likely voluntarily resigning after making substantial project contributions.\n\n- All employees with seven projects left the company. The interquartile range for those with six or seven projects was 255 to 295 hours per month, significantly higher than other groups.\n\n- Optimal employee performance is observed with 3–4 projects, evidenced by a low turnover rate among these groups.\n\n- Assuming a standard work week of 40 hours and two weeks of vacation per year:\n  - The average monthly working hours for Monday–Friday employees is 166.67 hours.\n  - Except for those working on two projects, all other groups, including those who stayed, worked significantly more, suggesting potential overwork.","metadata":{}},{"cell_type":"markdown","source":"It might be natural that people who work on more projects would also work longer hours. The mean hours of each group (stayed and left) increasing with number of projects worked. However, a few things stand out from this plot.\n\n1. There are two categories of employees who have left the organization: (A) those who worked significantly fewer hours compared to their peers with similar project involvement, and (B) those who worked considerably more. Within group A, it's plausible that some were terminated. Alternatively, this group might also include individuals who had already resigned and were given reduced workload as they approached their departure. As for group B, it's likely they voluntarily resigned. Employees in group B probably made substantial contributions to their projects, potentially being key contributors.\n\n2. All employees who had seven projects left the company. The interquartile range for this group, as well as for those who left with six projects, was approximately 255 to 295 hours per month, significantly higher than any other group.\n\n3. Employees appear to perform optimally when working on 3–4 projects. The ratio of those who left to those who stayed is notably low within these groups.\n\n4. Assuming a standard work week of 40 hours and two weeks of vacation per year, the average monthly working hours for employees working Monday–Friday can be calculated as follows: 50 weeks * 40 hours per week / 12 months = 166.67 hours per month.This suggests that, apart from employees working on two projects, every other group, including those who remained with the company, worked significantly more hours than this average. This observation implies that employees may be experiencing overwork.","metadata":{}},{"cell_type":"code","source":"# Let's confirm that every employee with 7 projects left the company\ndf1[df1['number_project']==7]['left'].value_counts()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T17:18:03.156551Z","iopub.execute_input":"2024-09-03T17:18:03.156884Z","iopub.status.idle":"2024-09-03T17:18:03.169451Z","shell.execute_reply.started":"2024-09-03T17:18:03.156854Z","shell.execute_reply":"2024-09-03T17:18:03.16769Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We have confirmed that all employees with 7 project did leave.\n\nNext, the examination of average mothly hours versus the satisfaction levels.","metadata":{}},{"cell_type":"code","source":"# Scatterplot of 'average_monthly_hours' versus 'satisfaction_level', comparing employees who stayed versus those who left\nplt.figure(figsize=(18, 10))\nscatter=sns.scatterplot(\n    data=df1, x='average_monthly_hours', y='satisfaction_level', \n    hue='left', palette={0: '#377eb8', 1: '#ff7f00'}, alpha=0.5\n)\nvline = plt.axvline(x=166.67, color='#e41a1c', label='166.67 hrs/mo', ls='--')\n\n# Create custom legend\nhandles, _ = scatter.get_legend_handles_labels()\nplt.legend(handles=[vline] + handles, labels=['166.67 hrs/mo', 'Stayed', 'Left'])\n\nplt.title('Monthly hours by evaluation', fontsize=16)\nplt.show()","metadata":{"id":"UCVs81NILbhn","execution":{"iopub.status.busy":"2024-09-03T17:18:03.171113Z","iopub.execute_input":"2024-09-03T17:18:03.171572Z","iopub.status.idle":"2024-09-03T17:18:04.158918Z","shell.execute_reply.started":"2024-09-03T17:18:03.171506Z","shell.execute_reply":"2024-09-03T17:18:04.157591Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The scatterplot indicates a significant group of employees who worked between 240 and 315 hours per month. Working 315 hours per month equates to over 75 hours per week throughout the year. This heavy workload likely contributed to their near-zero satisfaction levels.\n\n\nThe plot also reveals another group of individuals who left the organization, despite having more regular working hours. Their satisfaction level was only about 0.4. It is challenging to determine the exact reasons for their departure. One possibility is that they felt pressured to work longer hours due to the high number of peers who did so, potentially leading to lower satisfaction levels.\n\nFinally, there is a group who worked approximately 210 to 280 hours per month, with satisfaction levels ranging from about 0.7 to 0.9.\n\nThe unusual shape of data distributions indicates either synthetic or manipulatd data.\n","metadata":{}},{"cell_type":"markdown","source":"Let's visualize satisfaction levels by tenure.","metadata":{}},{"cell_type":"code","source":"# Setting figure and axes\nfig, ax = plt.subplots(1,2, figsize=(22,8))\n\n# Boxplot showing distributions of `satisfaction_level` by tenure, comparing employees who stayed versus those who left\nsns.boxplot(data=df1, x='satisfaction_level', y='tenure', hue='left', orient='h',ax=ax[0])\nax[0].invert_yaxis()\nax[0].set_title('Satisfaction by tenure', fontsize=16)\n\n# Histogram showing distribution of `tenure`, comparing employees who stayed versus those who left\ntenure_stay = df1[df1['left']==0]['tenure']\ntenure_left = df1[df1['left']==1]['tenure']\nsns.histplot(data=df1, x='tenure', hue='left', multiple='dodge', shrink=6, ax=ax[1])\nax[1].set_title('Distribution of tenure', fontsize=16)\nplt.show()","metadata":{"id":"cGitCvzvdbjF","execution":{"iopub.status.busy":"2024-09-03T17:18:04.160351Z","iopub.execute_input":"2024-09-03T17:18:04.160759Z","iopub.status.idle":"2024-09-03T17:18:05.459386Z","shell.execute_reply.started":"2024-09-03T17:18:04.160726Z","shell.execute_reply":"2024-09-03T17:18:05.45824Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- We can observed two general groups of people who left: dissatisfied with shorter tenures and satisfied with medium-length tenures.\n- Worth of investigation are four-year employees who left with significant low satisfaction level.\n- The employees with the longest tenure did not leave. Their satisfaction levels were similar to those of newer employees who remained with the company.\n- The histogram indicates that there are relatively few employees with longer tenure. It's possible that these individuals hold higher-ranking, higher-paid positions\n","metadata":{"id":"6TyBo1uxsSpc"}},{"cell_type":"markdown","source":"As the next step, we can calculate the mean and median satisfaction score of employees who left and who did not.\n","metadata":{"id":"lfo96dwwruZd"}},{"cell_type":"code","source":"# Calculation of mean & median satisfaction scores of employees who left and those whose stayed.\ndf1.groupby(['left'])['satisfaction_level'].agg(['mean','median'])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T17:18:05.461214Z","iopub.execute_input":"2024-09-03T17:18:05.461573Z","iopub.status.idle":"2024-09-03T17:18:05.479247Z","shell.execute_reply.started":"2024-09-03T17:18:05.461544Z","shell.execute_reply":"2024-09-03T17:18:05.47803Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As expected median and mean satisfaction scores of employees who left are lower than of those who stayed.\nWe can see the median is slightly higher above the mean in employees who stayed, that indicates that satisfaction score between those who stayed might be skewed to the left\n\nNext, we will examine salary levels for different tenures.","metadata":{}},{"cell_type":"code","source":"# Histograms of long and short tenures\n\nfig, ax =plt.subplots(1,2,figsize=(22,8))\n\n# Short-tenurd employees\nshort_tenure = df1[df1['tenure'] < 7]\n# Long-tenured employees\nlong_tenure = df1[df1['tenure'] > 6]\n\n# Define custom colors for the hue categories\ncustom_palette = {'low': 'skyblue', 'medium': 'orange', 'high': 'green'}\n\n# Short-tenured histogram\nsns.histplot(\n    data=short_tenure, x='tenure', hue='salary', discrete=True,\n    hue_order=['low', 'medium', 'high'], palette=custom_palette, \n    multiple='dodge', shrink=0.5, ax=ax[0]\n)\nax[0].set_title('Salary Histogram of Short-Tenured Employees', fontsize=16)\n\n# Long-tenured histogram\nsns.histplot(\n    data=long_tenure, x='tenure', hue='salary', discrete=True,\n    hue_order=['low', 'medium', 'high'],palette=custom_palette, \n    multiple='dodge', shrink=0.4, ax=ax[1]\n)\nax[1].set_title('Salary Histogram of Long-Tenured Employees', fontsize=16)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:05.481014Z","iopub.execute_input":"2024-09-03T17:18:05.48185Z","iopub.status.idle":"2024-09-03T17:18:06.289209Z","shell.execute_reply.started":"2024-09-03T17:18:05.481818Z","shell.execute_reply":"2024-09-03T17:18:06.287988Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The plots above indicate that long-tenured employees were not predominantly higher-paid employees.\n\nNext, we explore whether there is a correlation between working long hours and receiving high evaluation scores by help of scatterplot.","metadata":{}},{"cell_type":"code","source":"# Scatterplot of average_monthly_hours versus last_evaluation\nplt.figure(figsize=(16, 8))\nscatter = sns.scatterplot(data=df1, x='average_monthly_hours', y='last_evaluation', hue='left',\n                          palette={0: '#377eb8', 1: '#ff7f00'}, alpha=0.5)\nvline = plt.axvline(x=166.67, color='#e41a1c', label='166.67 hrs/mo', ls='--')\n\n# Create custom legend\nhandles, _ = scatter.get_legend_handles_labels()\nplt.legend(handles=[vline] + handles, labels=['166.67 hrs/mo', 'Stayed', 'Left'])\n\nplt.title('Monthly hours by evaluation', fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:06.290647Z","iopub.execute_input":"2024-09-03T17:18:06.291012Z","iopub.status.idle":"2024-09-03T17:18:07.326759Z","shell.execute_reply.started":"2024-09-03T17:18:06.290978Z","shell.execute_reply":"2024-09-03T17:18:07.324903Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- The scatterplot reveals two distinct groups of employees who left: those who were productive and overworked, and those who worked slightly below average of 166.67 hours per month with lower evaluation scores.\n- A correlation between hours worked and evaluation score appears evident.\n- However, the upper left quadrant of the plot has a low concentration of employees, but working long hours does not ensure high evaluation score.\n- Notably, most people in the company work well beyond 167 hours per month.\n\nNow, we will examine whether employees that worked very long hours were promoted in the last five years.","metadata":{}},{"cell_type":"code","source":"# Scatterpolot - examination of relationship between 'average_montthly_hours' and 'promotion in last 5years'\nplt.figure(figsize=(17, 4))\nscatter = sns.scatterplot(data=df1, x='average_monthly_hours', y='promotion_last_5years',\n                          hue='left', alpha=0.5)\nvline = plt.axvline(x=166.67, color='#ff6361', ls='--', label='166.67 hrs./mo.')\n\n# Create custom legend\nhandles, _ = scatter.get_legend_handles_labels()\nplt.legend(handles=[vline] + handles, labels=['166.67 hrs./mo.', 'Stayed', 'Left'])\n\nplt.title(\"Monthly hours by promotion last 5 years\", fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:07.32837Z","iopub.execute_input":"2024-09-03T17:18:07.328779Z","iopub.status.idle":"2024-09-03T17:18:08.159214Z","shell.execute_reply.started":"2024-09-03T17:18:07.328746Z","shell.execute_reply":"2024-09-03T17:18:08.157624Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can observe the following from the plot:\n-     very few employees who got promoted in last 5 years left the company\n-     employees that worked longest hours they all left\n-     very few employees who worked long hours were promoted\n\nNext, we inspect how the employees who left are distributed across departments.","metadata":{}},{"cell_type":"code","source":"df1['department'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:08.160855Z","iopub.execute_input":"2024-09-03T17:18:08.161235Z","iopub.status.idle":"2024-09-03T17:18:08.171918Z","shell.execute_reply.started":"2024-09-03T17:18:08.161205Z","shell.execute_reply":"2024-09-03T17:18:08.170684Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Histogram for department distribution of employees who left\nplt.figure(figsize=(8,6))\nsns.histplot(data=df1, x='department', hue='left', multiple='dodge',\n            shrink=0.5, hue_order=[0,1])\nplt.xticks(rotation=45)\nplt.legend(labels=['left', 'stayed'])\nplt.title('People who left/stayed by department', fontsize=16);","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:08.173455Z","iopub.execute_input":"2024-09-03T17:18:08.173829Z","iopub.status.idle":"2024-09-03T17:18:08.619901Z","shell.execute_reply.started":"2024-09-03T17:18:08.173779Z","shell.execute_reply":"2024-09-03T17:18:08.618647Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There doesn't appear to be any department with a significantly different proportion of employees who left compared to those who stayed","metadata":{}},{"cell_type":"markdown","source":"At the end we check for correlation between predictors in the data.\n","metadata":{}},{"cell_type":"code","source":"# Correlation heatmap\nplt.figure(figsize=(12, 10))\nsns.heatmap(df0.corr(numeric_only=True), vmin=-1, vmax=1, annot=True, cmap=\"coolwarm\")\nplt.title('Correlation Heatmap', fontsize=16, pad=12)\nplt.xticks(rotation=25)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:08.621305Z","iopub.execute_input":"2024-09-03T17:18:08.621651Z","iopub.status.idle":"2024-09-03T17:18:09.200974Z","shell.execute_reply.started":"2024-09-03T17:18:08.621623Z","shell.execute_reply":"2024-09-03T17:18:09.199354Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The correlation heatmap reveals that the number of projects, monthly hours, and evaluation scores all exhibit some positive correlation with each other. Additionally, it shows that an employee's likelihood of leaving is negatively correlated with their satisfaction level.","metadata":{}},{"cell_type":"markdown","source":"### EDA Insights","metadata":{"id":"DeTmNVlAANLd"}},{"cell_type":"markdown","source":"It seems that employees are leaving the company due to poor management. Departure is associated with longer working hours, numerous projects, and generally lower satisfaction levels. Working long hours without receiving promotions or good evaluation scores can be demoralizing. There is a significant group of employees likely experiencing burnout. Additionally, employees who have been with the company for more than six years tend to stay.","metadata":{"id":"sQT8YqymD-yL"}},{"cell_type":"markdown","source":"# paCe: Construct Stage\n- Determine which models are most appropriate\n- Construct the model\n- Confirm model assumptions\n- Evaluate model results to determine how well your model fits the data\n","metadata":{"id":"Lca9c8XON8lc"}},{"cell_type":"markdown","source":"## Model Building\n\n\n### Identify the type of prediction task.","metadata":{"id":"QxCXjdpSaB8A"}},{"cell_type":"markdown","source":"Our goal is to predict whether an employee will leave the company, which is a categorical outcome variable. Therefore, this task involves classification. Specifically, it involves binary classification, as the outcome variable \"left\" can either be 1 (indicating the employee left) or 0 (indicating the employee did not leave).","metadata":{"id":"YcCoHhnKcEov"}},{"cell_type":"markdown","source":"### Identify the types of models most appropriate for this task.","metadata":{"id":"yeUlo3W0ais-"}},{"cell_type":"markdown","source":"Since the variable we want to predict is categorical, we could either build a Logistic Regression model, or a Tree-based ML model.","metadata":{"id":"A0Mo6f9tOchC"}},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{"id":"OrW2oXy4OfD1"}},{"cell_type":"markdown","source":"Logistic Regression - binary classification\n\nWe need to encode the non-numeric variables, department and salary.\n\n- department is suitable for dummy process, because it's categorical variable.\n- salary is suitable for the conversion of the levels to 0-2, not for dummy though, bacause it's categorical and ordinal, so there's an order(hierarchy) in categories.","metadata":{"id":"UePZZyi_Okdz"}},{"cell_type":"code","source":"# Copy DataFrame\ndf_enc =df1.copy()\n\n# Encode the salary as an ordinal numeric\ndf_enc['salary'] = pd.Categorical(df_enc['salary'], \n                                  categories=['low', 'medium', 'high'], ordered=True).codes\n# Dummy encode the department\ndf_enc = pd.get_dummies(df_enc, columns=['department'])\ndf_enc.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:09.214694Z","iopub.execute_input":"2024-09-03T17:18:09.215153Z","iopub.status.idle":"2024-09-03T17:18:09.246313Z","shell.execute_reply.started":"2024-09-03T17:18:09.215116Z","shell.execute_reply":"2024-09-03T17:18:09.245023Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We create a heatmap to visualize how correlated variables are.","metadata":{}},{"cell_type":"code","source":"# Heatmap with correlation of variables we've chosen based on the previous heatmap observations.\nplt.figure(figsize=(8, 6))\n\n# Use a colorblind-friendly colormap\nsns.heatmap(\n    df_enc[['satisfaction_level', 'last_evaluation', 'number_project',\n            'average_monthly_hours', 'tenure']].corr(numeric_only=True),\n    annot=True, cmap='cividis'\n)\nplt.xticks(rotation=25)\nplt.title('Correlation Heatmap', fontsize=16);","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:09.247824Z","iopub.execute_input":"2024-09-03T17:18:09.24829Z","iopub.status.idle":"2024-09-03T17:18:09.700161Z","shell.execute_reply.started":"2024-09-03T17:18:09.248249Z","shell.execute_reply":"2024-09-03T17:18:09.699022Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we visualize number of employees across department, comparing leavers and those who stayed.","metadata":{}},{"cell_type":"markdown","source":"As we've chosen logistic regression, it would be wise to remove the outliers in the 'tenure' identified earlier, since the model is sensitive to them.","metadata":{}},{"cell_type":"code","source":"# Removal of outliers rows from 'tenure', by selecting those are not actual outliers. \n# Saving them to new DataFrame.\ndf_logreg = df_enc[(df_enc['tenure'] >= lower_bounds) & (df_enc['tenure'] <= upper_bounds)]\ndf_logreg.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:09.701504Z","iopub.execute_input":"2024-09-03T17:18:09.701832Z","iopub.status.idle":"2024-09-03T17:18:09.72664Z","shell.execute_reply.started":"2024-09-03T17:18:09.701804Z","shell.execute_reply":"2024-09-03T17:18:09.725327Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we isolate dependent (outcome) variable, we are trying to predict with the model.","metadata":{}},{"cell_type":"code","source":"# Outcome (Y) variable isolation\ny = df_logreg['left']\ny.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T17:18:09.727888Z","iopub.execute_input":"2024-09-03T17:18:09.728275Z","iopub.status.idle":"2024-09-03T17:18:09.736545Z","shell.execute_reply.started":"2024-09-03T17:18:09.728246Z","shell.execute_reply":"2024-09-03T17:18:09.73538Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We select features we want to use in our model. It should be varible that help us predict Y-variable.","metadata":{}},{"cell_type":"code","source":"# Selecting only X variables(predictors) - removing the Y\nX = df_logreg.drop('left', axis=1)\nX.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:09.738161Z","iopub.execute_input":"2024-09-03T17:18:09.738572Z","iopub.status.idle":"2024-09-03T17:18:09.764732Z","shell.execute_reply.started":"2024-09-03T17:18:09.738539Z","shell.execute_reply":"2024-09-03T17:18:09.76362Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we can split the data into training and testing set.\n\nWe need to stratify Y-variable as the classes are not balanced.","metadata":{}},{"cell_type":"code","source":"# Splitting the data - training & testing set - using 75/25 split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, stratify=y, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:09.766091Z","iopub.execute_input":"2024-09-03T17:18:09.766433Z","iopub.status.idle":"2024-09-03T17:18:09.780877Z","shell.execute_reply.started":"2024-09-03T17:18:09.766404Z","shell.execute_reply":"2024-09-03T17:18:09.779424Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The construction of Logistic Regression model and fitting it to the training set.","metadata":{}},{"cell_type":"code","source":"# Construct & fit of Log. Reg. model\nlog_clf = LogisticRegression(random_state=42, max_iter=500).fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:09.783203Z","iopub.execute_input":"2024-09-03T17:18:09.783672Z","iopub.status.idle":"2024-09-03T17:18:10.120839Z","shell.execute_reply.started":"2024-09-03T17:18:09.783635Z","shell.execute_reply":"2024-09-03T17:18:10.11951Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's test logistic regression model on the test set to make predictions.\n\nAfter that we create confusion matrix to visualize the results.","metadata":{}},{"cell_type":"code","source":"# Predicting on X_test set.\ny_pred = log_clf.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:10.122442Z","iopub.execute_input":"2024-09-03T17:18:10.123573Z","iopub.status.idle":"2024-09-03T17:18:10.138195Z","shell.execute_reply.started":"2024-09-03T17:18:10.123526Z","shell.execute_reply":"2024-09-03T17:18:10.136553Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Confusion matrix\n\n# Computation of values \ncm = confusion_matrix(y_test, y_pred, labels=log_clf.classes_)\n\n# Display creation for CM\nlog_disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                                 display_labels=log_clf.classes_)\n\n# Confusion Matrix plot\nlog_disp.plot(values_format='', cmap='cividis')\nplt.title('Logistic regression - test set');","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:10.140761Z","iopub.execute_input":"2024-09-03T17:18:10.142494Z","iopub.status.idle":"2024-09-03T17:18:10.509158Z","shell.execute_reply.started":"2024-09-03T17:18:10.142431Z","shell.execute_reply":"2024-09-03T17:18:10.507624Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The upper left quadrant - True negatives, the upper right quadrant - False positives, the bottom left - False negatives, the bottom right True positives.\n\n**True negatives:** the number of people who did not leave, that the model accurately predicted as not leaving.\n\n**False positives:** the number of people whot did not leave, that the model inaccurately predicted  as leaving.\n\n**True positives:** the number of people who left, that model accurately predicted as leaving.\n\n**False negatives:** the number of people who left, that model inaccurately predicted as not leaving.\n\nIn case of a perfect model we would see all true negative and true positive values, and not false \npositive and false negative.","metadata":{}},{"cell_type":"markdown","source":"Now, we can create classification report that yields recall, precision, accuracy and f1 metrics to evaluate pefrmances of the model itself.","metadata":{}},{"cell_type":"markdown","source":"Before that we need to confirm class balance in the 'left' field. As we're dealing with binary classification task, the way of interpretation of the accuracy metrics depends on the class balance.","metadata":{}},{"cell_type":"code","source":"left_percentage = round(df_logreg['left'].value_counts(normalize=True)*100,2)\nleft_percentage","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T17:18:10.510633Z","iopub.execute_input":"2024-09-03T17:18:10.511015Z","iopub.status.idle":"2024-09-03T17:18:10.521324Z","shell.execute_reply.started":"2024-09-03T17:18:10.510981Z","shell.execute_reply":"2024-09-03T17:18:10.520041Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The data is not perfectly balanced, but it's not too imbalanced neither. There is a 83/17 split, in this case we do not need to resample the data to make it more balanced.","metadata":{}},{"cell_type":"code","source":"# Classification report\ntarget_names=['Predicted would not leave', 'Predicted would leave']\nprint(classification_report(y_test, y_pred, target_names=target_names))","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:10.522334Z","iopub.execute_input":"2024-09-03T17:18:10.522658Z","iopub.status.idle":"2024-09-03T17:18:10.54597Z","shell.execute_reply.started":"2024-09-03T17:18:10.52263Z","shell.execute_reply":"2024-09-03T17:18:10.544797Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The report yielded that the model achieved on employees who would not leave a f1 score of 80%, recall of 82%, precision of 79% (all weighted averages) and accuracy score of 80%.","metadata":{}},{"cell_type":"markdown","source":"### Tree-based Model","metadata":{}},{"cell_type":"markdown","source":"The implementation of Decision Tree and Random Forest","metadata":{}},{"cell_type":"markdown","source":"Isolation of the dependant variable","metadata":{}},{"cell_type":"code","source":"# Isolate the Y variable\ny = df_enc['left']\n\ny.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:10.547391Z","iopub.execute_input":"2024-09-03T17:18:10.54783Z","iopub.status.idle":"2024-09-03T17:18:10.557554Z","shell.execute_reply.started":"2024-09-03T17:18:10.547793Z","shell.execute_reply":"2024-09-03T17:18:10.556198Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Select the features\nX = df_enc.drop('left',axis=1)\n\nX.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:10.559337Z","iopub.execute_input":"2024-09-03T17:18:10.559741Z","iopub.status.idle":"2024-09-03T17:18:10.584756Z","shell.execute_reply.started":"2024-09-03T17:18:10.559706Z","shell.execute_reply":"2024-09-03T17:18:10.583726Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split the data 75/25\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:10.586128Z","iopub.execute_input":"2024-09-03T17:18:10.586676Z","iopub.status.idle":"2024-09-03T17:18:10.599428Z","shell.execute_reply.started":"2024-09-03T17:18:10.586634Z","shell.execute_reply":"2024-09-03T17:18:10.598276Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Decision Tree - 1st round","metadata":{}},{"cell_type":"markdown","source":"Construct a decision tree model and set up cross-validated grid-search to search for the best model parameters.","metadata":{}},{"cell_type":"code","source":"# Model instantiation\ndt1_clf = DecisionTreeClassifier(random_state=0)\n\n#Dictionary of hyperparameters to search over - cross-validation\ncv_params = {'max_depth':[4, 6, 8, None],\n             'min_samples_leaf': [2, 5, 1],\n             'min_samples_split': [2, 4, 6]\n            }\n# Dictionary of scoring metrics\nscoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n\n# GridSearch instantiation\ntree1 = GridSearchCV(dt1_clf, cv_params, scoring=scoring, cv=4, refit='roc_auc')","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:10.600819Z","iopub.execute_input":"2024-09-03T17:18:10.601276Z","iopub.status.idle":"2024-09-03T17:18:10.608884Z","shell.execute_reply.started":"2024-09-03T17:18:10.601241Z","shell.execute_reply":"2024-09-03T17:18:10.607507Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Fit the DT model to the training data","metadata":{}},{"cell_type":"code","source":"%%time\ntree1.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:10.610242Z","iopub.execute_input":"2024-09-03T17:18:10.610573Z","iopub.status.idle":"2024-09-03T17:18:15.884743Z","shell.execute_reply.started":"2024-09-03T17:18:10.610547Z","shell.execute_reply":"2024-09-03T17:18:15.883666Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, we can proceed with identification of best parameters & the best AUC score of DT model on the training set.","metadata":{}},{"cell_type":"code","source":"# Best parameters\ntree1.best_params_","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:15.886061Z","iopub.execute_input":"2024-09-03T17:18:15.886368Z","iopub.status.idle":"2024-09-03T17:18:15.893603Z","shell.execute_reply.started":"2024-09-03T17:18:15.886343Z","shell.execute_reply":"2024-09-03T17:18:15.892395Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Best AUC score\ntree1.best_score_","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:15.895304Z","iopub.execute_input":"2024-09-03T17:18:15.89574Z","iopub.status.idle":"2024-09-03T17:18:15.916422Z","shell.execute_reply.started":"2024-09-03T17:18:15.895702Z","shell.execute_reply":"2024-09-03T17:18:15.915085Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Decision tree model achieved strong AUC score, that shows it can predict leaving employees very good.\n\nThe following function helps us retrieve cross-validation results from GridSearchCV, representing the average performance across all validation folds.","metadata":{}},{"cell_type":"code","source":"def make_results(model_name: str, model_object, metric: str) -> pd.DataFrame:\n    '''  \n    Arguments:\n        model_name (string): what you want the model to be called in the output table\n        model_object: a fit GridSearchCV object\n        metric (string): precision, recall, f1, accuracy, or auc\n  \n    Returns a pandas df with the F1, recall, precision, accuracy, and auc scores\n    for the model with the best mean 'metric' score across all validation folds.  \n    '''\n    # Mapping input metric to actual metric name in GridSearchCV\n    metric_dict = {\n        'auc': 'mean_test_roc_auc',\n        'precision': 'mean_test_precision',\n        'recall': 'mean_test_recall',\n        'f1': 'mean_test_f1',\n        'accuracy': 'mean_test_accuracy'\n    }\n\n    # Extract CV results into a DataFrame\n    cv_results = pd.DataFrame(model_object.cv_results_)\n\n    # Identify the best estimator based on the chosen metric\n    best_index = cv_results[metric_dict[metric]].idxmax()\n    best_estimator_results = cv_results.loc[best_index]\n\n    # Extract relevant metrics\n    results = {\n        'model': model_name,\n        'precision': best_estimator_results['mean_test_precision'],\n        'recall': best_estimator_results['mean_test_recall'],\n        'F1': best_estimator_results['mean_test_f1'],\n        'accuracy': best_estimator_results['mean_test_accuracy'],\n        'auc': best_estimator_results['mean_test_roc_auc']\n    }\n\n    # Create and return the results DataFrame\n    return pd.DataFrame([results])","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:15.918075Z","iopub.execute_input":"2024-09-03T17:18:15.918534Z","iopub.status.idle":"2024-09-03T17:18:15.93338Z","shell.execute_reply.started":"2024-09-03T17:18:15.918495Z","shell.execute_reply":"2024-09-03T17:18:15.93189Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Getting all the scores with 'make_results' funtion","metadata":{}},{"cell_type":"code","source":"# Get all scores\ntree1_cv_results = make_results('decision tree1 cv', tree1, 'auc')\ntree1_cv_results","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T17:18:15.938451Z","iopub.execute_input":"2024-09-03T17:18:15.939309Z","iopub.status.idle":"2024-09-03T17:18:15.964687Z","shell.execute_reply.started":"2024-09-03T17:18:15.939266Z","shell.execute_reply":"2024-09-03T17:18:15.962841Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### XGBoost - 1st round","metadata":{}},{"cell_type":"code","source":"xgb1_clf = XGBClassifier(objective='binary:logistic', random_state=0)\n\ncv_params = {'max_depth': [4,6], \n             'min_child_weight': [3,5],\n             'learning_rate': [0.1, 0.2, 0.3],\n             'n_estimators': [5,10,15],\n             #'subsample': [0.7],\n             }    \n\nscoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n\nxgb1 = GridSearchCV(xgb1_clf, cv_params, scoring=scoring, cv=5, refit='roc_auc')","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:15.966249Z","iopub.execute_input":"2024-09-03T17:18:15.966691Z","iopub.status.idle":"2024-09-03T17:18:15.974918Z","shell.execute_reply.started":"2024-09-03T17:18:15.96665Z","shell.execute_reply":"2024-09-03T17:18:15.973038Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# Fit the model\nxgb1.fit(X_train, y_train)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T17:18:15.97651Z","iopub.execute_input":"2024-09-03T17:18:15.977177Z","iopub.status.idle":"2024-09-03T17:18:29.76135Z","shell.execute_reply.started":"2024-09-03T17:18:15.977132Z","shell.execute_reply":"2024-09-03T17:18:29.7602Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check best AUC score on CV\nxgb1.best_score_","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:29.762892Z","iopub.execute_input":"2024-09-03T17:18:29.763355Z","iopub.status.idle":"2024-09-03T17:18:29.770667Z","shell.execute_reply.started":"2024-09-03T17:18:29.763316Z","shell.execute_reply":"2024-09-03T17:18:29.769588Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check best params\nxgb1.best_params_","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T17:18:29.771878Z","iopub.execute_input":"2024-09-03T17:18:29.772325Z","iopub.status.idle":"2024-09-03T17:18:29.786079Z","shell.execute_reply.started":"2024-09-03T17:18:29.772291Z","shell.execute_reply":"2024-09-03T17:18:29.784811Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get all CV scores\nxgb1_cv_results = make_results('xgboost1 cv', xgb1, 'auc')\nxgb1_cv_results","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:29.787707Z","iopub.execute_input":"2024-09-03T17:18:29.788151Z","iopub.status.idle":"2024-09-03T17:18:29.807004Z","shell.execute_reply.started":"2024-09-03T17:18:29.788111Z","shell.execute_reply":"2024-09-03T17:18:29.805662Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Random forest - 1st round","metadata":{}},{"cell_type":"markdown","source":"Construct a Random forest model and set up cross-validated grid-search to search for the best model parameters.","metadata":{}},{"cell_type":"code","source":"# Model instantiation\nrf1_clf = RandomForestClassifier(random_state=0)\n\n# Dictionary of hyperparameters to search over - cross-validation\ncv_params = {'max_depth': [3,5, None], \n             'max_features': [1.0],\n             'max_samples': [0.7, 1.0],\n             'min_samples_leaf': [1,2,3],\n             'min_samples_split': [2,3,4],\n             'n_estimators': [300, 500],\n            }\n# Dictionary of scoring metrics\nscoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n\n# GridSearch instantiation\nrf1 = GridSearchCV(rf1_clf, cv_params, scoring=scoring, cv=4, refit='roc_auc')","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:18:29.808582Z","iopub.execute_input":"2024-09-03T17:18:29.809014Z","iopub.status.idle":"2024-09-03T17:18:29.818584Z","shell.execute_reply.started":"2024-09-03T17:18:29.808976Z","shell.execute_reply":"2024-09-03T17:18:29.817535Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# Fit the model\nrf1.fit(X_train, y_train)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T17:18:29.82038Z","iopub.execute_input":"2024-09-03T17:18:29.820814Z","iopub.status.idle":"2024-09-03T17:49:42.219053Z","shell.execute_reply.started":"2024-09-03T17:18:29.820775Z","shell.execute_reply":"2024-09-03T17:49:42.217761Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we define the function to pickle and read in the model.","metadata":{}},{"cell_type":"markdown","source":"We use functions above to save the model in a pickle file and then read it in.","metadata":{}},{"cell_type":"markdown","source":"We can proceed with identification of best parameters & the best AUC score of RF model on the training set.","metadata":{}},{"cell_type":"code","source":"import pickle\n# the path when we want to save the model\npath = r'C:\\Users\\bajdikm\\OneDrive - British Standards Institution\\Desktop\\Marek\\Data Analyst_Scientist\\Advanced Data Analytics\\TikTok\\6 ML'","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:49:42.220988Z","iopub.execute_input":"2024-09-03T17:49:42.221411Z","iopub.status.idle":"2024-09-03T17:49:42.227173Z","shell.execute_reply.started":"2024-09-03T17:49:42.221371Z","shell.execute_reply":"2024-09-03T17:49:42.225722Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def write_pickle(path, model_object, save_as:str):\n    '''\n    In: \n        path:         path of folder where you want to save the pickle\n        model_object: a model you want to pickle\n        save_as:      filename for how you want to save the model\n\n    Out: A call to pickle the model in the folder indicated\n    '''    \n\n    with open(path + save_as + '.pickle', 'wb') as to_write:\n        pickle.dump(model_object, to_write)\nprint(f\"Model saved to {path}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:49:42.228831Z","iopub.execute_input":"2024-09-03T17:49:42.229355Z","iopub.status.idle":"2024-09-03T17:49:42.244051Z","shell.execute_reply.started":"2024-09-03T17:49:42.229313Z","shell.execute_reply":"2024-09-03T17:49:42.242893Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def read_pickle(path, saved_model_name:str):\n    '''\n    In: \n        path:             path to folder where you want to read from\n        saved_model_name: filename of pickled model you want to read in\n\n    Out: \n        model: the pickled model \n    '''\n    with open(path + saved_model_name + '.pickle', 'rb') as to_read:\n        model = pickle.load(to_read)\n\n    return model\nprint(\"Model loaded successfully\")","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:49:42.24552Z","iopub.execute_input":"2024-09-03T17:49:42.245955Z","iopub.status.idle":"2024-09-03T17:49:42.266107Z","shell.execute_reply.started":"2024-09-03T17:49:42.245901Z","shell.execute_reply":"2024-09-03T17:49:42.264808Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Write pickle\nwrite_pickle(path, rf1, 'hr_rf1')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T17:49:42.267963Z","iopub.execute_input":"2024-09-03T17:49:42.268558Z","iopub.status.idle":"2024-09-03T17:49:42.302473Z","shell.execute_reply.started":"2024-09-03T17:49:42.268526Z","shell.execute_reply":"2024-09-03T17:49:42.301298Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read pickle\nrf1 = read_pickle(path, 'hr_rf1')","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:49:42.304175Z","iopub.execute_input":"2024-09-03T17:49:42.304544Z","iopub.status.idle":"2024-09-03T17:49:42.331154Z","shell.execute_reply.started":"2024-09-03T17:49:42.304506Z","shell.execute_reply":"2024-09-03T17:49:42.329726Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check best AUC score on CV\nrf1.best_score_","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:49:42.332725Z","iopub.execute_input":"2024-09-03T17:49:42.333191Z","iopub.status.idle":"2024-09-03T17:49:42.346744Z","shell.execute_reply.started":"2024-09-03T17:49:42.333149Z","shell.execute_reply":"2024-09-03T17:49:42.345605Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check best parameters\nrf1.best_params_","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:49:42.348264Z","iopub.execute_input":"2024-09-03T17:49:42.348601Z","iopub.status.idle":"2024-09-03T17:49:42.360243Z","shell.execute_reply.started":"2024-09-03T17:49:42.348574Z","shell.execute_reply":"2024-09-03T17:49:42.359201Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's collect the scores on the training data for all three models.","metadata":{}},{"cell_type":"code","source":"# Get all CV scores\nrf1_cv_results = make_results('random forest1 cv', rf1, 'auc')\n\n# Combine all results into a single DataFrame\nall_results = pd.concat([tree1_cv_results, rf1_cv_results, xgb1_cv_results], ignore_index=True)\n\nprint(all_results)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T17:49:42.361901Z","iopub.execute_input":"2024-09-03T17:49:42.362639Z","iopub.status.idle":"2024-09-03T17:49:42.379046Z","shell.execute_reply.started":"2024-09-03T17:49:42.362598Z","shell.execute_reply":"2024-09-03T17:49:42.377968Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The XGBoost model generally outperformed the decision tree and random forest models across most evaluation metrics, with only a slight decrease in recall. The random forest model also showed better performance than the decision tree model in most metrics, with a negligible difference in recall. Therefore, XGBoost appears to be the best-performing model overall, followed by the random forest and then the decision tree model.","metadata":{}},{"cell_type":"markdown","source":"Next, we evaluate XGBoost and Random forest on the test set.","metadata":{}},{"cell_type":"markdown","source":"We use a function to obtain all scores from the model's predictions, allowing for quick assessment and comparison of different models or configurations.","metadata":{}},{"cell_type":"code","source":"def get_scores(model_name: str, model, X_test_data, y_test_data) -> pd.DataFrame:\n  \n    '''\n    Generate a table of test scores.\n\n    Parameters:\n        model_name (str): How you want your model to be named in the output table\n        model: A fit GridSearchCV object\n        X_test_data: numpy array of X_test data\n        y_test_data: numpy array of y_test data\n\n    Returns:\n        pd.DataFrame: DataFrame containing precision, recall, f1, accuracy, and AUC scores for the model\n     '''\n    # Make predictions using the best estimator\n    preds = model.best_estimator_.predict(X_test_data)\n\n    # Calculate the various evaluation metrics\n    metrics = {\n        'model': model_name,\n        'precision': precision_score(y_test_data, preds),\n        'recall': recall_score(y_test_data, preds),\n        'f1': f1_score(y_test_data, preds),\n        'accuracy': accuracy_score(y_test_data, preds),\n        'AUC': roc_auc_score(y_test_data, preds)\n    }\n\n    # Create and return the results DataFrame\n    return pd.DataFrame([metrics])","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:49:42.380619Z","iopub.execute_input":"2024-09-03T17:49:42.381632Z","iopub.status.idle":"2024-09-03T17:49:42.388716Z","shell.execute_reply.started":"2024-09-03T17:49:42.381599Z","shell.execute_reply":"2024-09-03T17:49:42.387457Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It represents performance on a single test set.","metadata":{}},{"cell_type":"markdown","source":"Now, apply the best performing models to make predictions on the test set.","metadata":{}},{"cell_type":"code","source":"# Get xgb1 scores\nxgb1_test_scores = get_scores('xgboost test', xgb1, X_test, y_test)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T17:49:42.390232Z","iopub.execute_input":"2024-09-03T17:49:42.391249Z","iopub.status.idle":"2024-09-03T17:49:42.431538Z","shell.execute_reply.started":"2024-09-03T17:49:42.391209Z","shell.execute_reply":"2024-09-03T17:49:42.430307Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get random forest1 scores\nrf1_test_scores = get_scores('random forest1 test', rf1, X_test, y_test)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T17:49:42.432634Z","iopub.execute_input":"2024-09-03T17:49:42.432924Z","iopub.status.idle":"2024-09-03T17:49:42.570964Z","shell.execute_reply.started":"2024-09-03T17:49:42.432899Z","shell.execute_reply":"2024-09-03T17:49:42.569956Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compare both scores\nrf1_vs_xgb1 = pd.concat([rf1_test_scores, xgb1_test_scores], ignore_index=True)\nrf1_vs_xgb1","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T17:49:42.572351Z","iopub.execute_input":"2024-09-03T17:49:42.572684Z","iopub.status.idle":"2024-09-03T17:49:42.586306Z","shell.execute_reply.started":"2024-09-03T17:49:42.572657Z","shell.execute_reply":"2024-09-03T17:49:42.584738Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Overall Conclusion**\n\nBoth models perform exceptionally well, with slight differences in their metrics. The XGBoost model shows a higher precision and F1 score, while the Random Forest model exhibits marginally better recall and AUC. Given these results, the choice between the two models may depend on the specific priorities of the task:\n\n- If minimizing false positives is critical, the XGBoost model is preferable due to its higher precision.\n- If identifying true positives is more important, the Random Forest model is a better choice due to its higher recall and AUC.\n\nBoth models would be an excellent choice, but the XGBoost model has a slight edge in balanced performance as indicated by its F1 score and accuracy.","metadata":{}},{"cell_type":"markdown","source":"#### Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"We might be questioning the high evaluation scores, suspecting potential data leakage. Data leakage happens when information that should be excluded during training is used, either because it appears in the test set or because it includes data that wouldn't be available when the model is deployed. Training with leaked data can result in an unrealistically high score that cannot be replicated in a production environment.","metadata":{}},{"cell_type":"markdown","source":"In this case, it’s probable that the company won't have satisfaction levels reported for all employees. Additionally, the `average_monthly_hours` column might be a source of data leakage. If employees have already decided to quit, or if management has identified them to be fired, they may be working fewer hours.","metadata":{}},{"cell_type":"markdown","source":"In the first round we included all features. The next round feature engineering will be incorporated to improve models.","metadata":{}},{"cell_type":"markdown","source":"We now proceed with dropping of `satisfaction level` and create new feature capturing whether an employee is overworked. It will be binary variable called `overworked`.","metadata":{}},{"cell_type":"code","source":"df2 = df_enc.drop('satisfaction_level', axis=1)\ndf2.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T17:49:42.587758Z","iopub.execute_input":"2024-09-03T17:49:42.588188Z","iopub.status.idle":"2024-09-03T17:49:42.612552Z","shell.execute_reply.started":"2024-09-03T17:49:42.588151Z","shell.execute_reply":"2024-09-03T17:49:42.611325Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# New 'overworked' feature is for now assigned to\ndf2['overworked'] = df2['average_monthly_hours']\n\n# Inspection of max & min AVG monthly hours\nprint('Max hours:',df2['overworked'].max())\nprint('Min hours:',df2['overworked'].min())","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:49:42.614416Z","iopub.execute_input":"2024-09-03T17:49:42.614794Z","iopub.status.idle":"2024-09-03T17:49:42.622589Z","shell.execute_reply.started":"2024-09-03T17:49:42.614765Z","shell.execute_reply":"2024-09-03T17:49:42.621272Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The average number of monthly hours is approximately 166.67 for someone who works 50 weeks per year, 5 days per week, 8 hours per day.\n\nWe can define someone being overworked who works on average more than 175 hours per month.\n\n","metadata":{}},{"cell_type":"code","source":"# Define 'overworked' - working more than 175 hours per week.\ndf2['overworked'] = (df2['overworked'] > 175).astype(int)\n\ndf2['overworked'].head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T17:49:42.624426Z","iopub.execute_input":"2024-09-03T17:49:42.624864Z","iopub.status.idle":"2024-09-03T17:49:42.638674Z","shell.execute_reply.started":"2024-09-03T17:49:42.624827Z","shell.execute_reply":"2024-09-03T17:49:42.637534Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop the 'average_monthly_hours' column\ndf2 = df2.drop('average_monthly_hours', axis=1)\n\n# Display first few rows of resulting dataframe\ndf2.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T17:49:42.640453Z","iopub.execute_input":"2024-09-03T17:49:42.641322Z","iopub.status.idle":"2024-09-03T17:49:42.665967Z","shell.execute_reply.started":"2024-09-03T17:49:42.641281Z","shell.execute_reply":"2024-09-03T17:49:42.664877Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's isolate Y and X variables.","metadata":{}},{"cell_type":"code","source":"# outcome variable\ny = df2['left']\n\n# predictors\nX = df2.drop('left', axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:49:42.66751Z","iopub.execute_input":"2024-09-03T17:49:42.667852Z","iopub.status.idle":"2024-09-03T17:49:42.675003Z","shell.execute_reply.started":"2024-09-03T17:49:42.667824Z","shell.execute_reply":"2024-09-03T17:49:42.673798Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split the data, 75/25\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:49:42.676761Z","iopub.execute_input":"2024-09-03T17:49:42.677255Z","iopub.status.idle":"2024-09-03T17:49:42.698866Z","shell.execute_reply.started":"2024-09-03T17:49:42.677215Z","shell.execute_reply":"2024-09-03T17:49:42.697704Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Decision tree - Round 2","metadata":{}},{"cell_type":"code","source":"# Decision tree instance\ndt2_clf = DecisionTreeClassifier(random_state=0)\n\n# Cross-validation hyperparameters in dictionary\ncv_params={'max_depth':[4, 6, 8, None],\n             'min_samples_leaf': [2, 5, 1],\n             'min_samples_split': [2, 4, 6]\n          }\n# Scoring dictionary\nscoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n\n\n# GridSearch instance\ntree2 = GridSearchCV(dt2_clf, cv_params, scoring=scoring, cv=4, refit='roc_auc')","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:49:42.700268Z","iopub.execute_input":"2024-09-03T17:49:42.700624Z","iopub.status.idle":"2024-09-03T17:49:42.708027Z","shell.execute_reply.started":"2024-09-03T17:49:42.700588Z","shell.execute_reply":"2024-09-03T17:49:42.706792Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ntree2.fit(X_train, y_train)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T17:49:42.70949Z","iopub.execute_input":"2024-09-03T17:49:42.70991Z","iopub.status.idle":"2024-09-03T17:49:47.18033Z","shell.execute_reply.started":"2024-09-03T17:49:42.709873Z","shell.execute_reply":"2024-09-03T17:49:47.179268Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Best parameters\ntree2.best_params_","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:49:47.181689Z","iopub.execute_input":"2024-09-03T17:49:47.182056Z","iopub.status.idle":"2024-09-03T17:49:47.188555Z","shell.execute_reply.started":"2024-09-03T17:49:47.182025Z","shell.execute_reply":"2024-09-03T17:49:47.187443Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Best score\ntree2.best_score_","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:49:47.189815Z","iopub.execute_input":"2024-09-03T17:49:47.190164Z","iopub.status.idle":"2024-09-03T17:49:47.202773Z","shell.execute_reply.started":"2024-09-03T17:49:47.190134Z","shell.execute_reply":"2024-09-03T17:49:47.201738Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model performs exceptionally well, even in the absence of data on satisfaction levels and worked hours.\n\nNow we can have a look at the other scores.","metadata":{}},{"cell_type":"code","source":"# Get all CV scores\ntree2_cv_results = make_results('decision tree2 cv', tree2, 'auc')\ntree2_cv_results","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:49:47.20434Z","iopub.execute_input":"2024-09-03T17:49:47.204688Z","iopub.status.idle":"2024-09-03T17:49:47.22569Z","shell.execute_reply.started":"2024-09-03T17:49:47.20466Z","shell.execute_reply":"2024-09-03T17:49:47.224474Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Most of the scores are lower, but that's expected in regards of prior features reduction for the second round of tests.","metadata":{}},{"cell_type":"markdown","source":"#### Random forest - Round 2","metadata":{}},{"cell_type":"code","source":"# Model instantiation\nrf2_clf = RandomForestClassifier(random_state=0)\n\n# Dictionary of hyperparameters to search over - cross-validation\ncv_params = {'max_depth': [3,5, None], \n             'max_features': [1.0],\n             'max_samples': [0.7, 1.0],\n             'min_samples_leaf': [1,2,3],\n             'min_samples_split': [2,3,4],\n             'n_estimators': [300, 500],\n            }\n# Dictionary of scoring metrics\nscoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n\n# GridSearch instantiation\nrf2 = GridSearchCV(rf2_clf, cv_params, scoring=scoring, cv=4, refit='roc_auc')","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:49:47.227238Z","iopub.execute_input":"2024-09-03T17:49:47.227608Z","iopub.status.idle":"2024-09-03T17:49:47.236789Z","shell.execute_reply.started":"2024-09-03T17:49:47.227579Z","shell.execute_reply":"2024-09-03T17:49:47.235526Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# 7 minutes run time\nrf2.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T17:49:47.238438Z","iopub.execute_input":"2024-09-03T17:49:47.238797Z","iopub.status.idle":"2024-09-03T18:13:05.710682Z","shell.execute_reply.started":"2024-09-03T17:49:47.238767Z","shell.execute_reply":"2024-09-03T18:13:05.709613Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Write pickle\n#write_pickle(path, rf2, 'hr_rf2')","metadata":{"execution":{"iopub.status.busy":"2024-09-03T18:13:05.712182Z","iopub.execute_input":"2024-09-03T18:13:05.712541Z","iopub.status.idle":"2024-09-03T18:13:05.716884Z","shell.execute_reply.started":"2024-09-03T18:13:05.712513Z","shell.execute_reply":"2024-09-03T18:13:05.715849Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read in pickle\n#rf2 = read_pickle(path, 'hr_rf2')","metadata":{"execution":{"iopub.status.busy":"2024-09-03T18:13:05.718132Z","iopub.execute_input":"2024-09-03T18:13:05.718441Z","iopub.status.idle":"2024-09-03T18:13:05.740104Z","shell.execute_reply.started":"2024-09-03T18:13:05.718415Z","shell.execute_reply":"2024-09-03T18:13:05.739037Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check best params\nrf2.best_params_","metadata":{"execution":{"iopub.status.busy":"2024-09-03T18:13:05.752034Z","iopub.execute_input":"2024-09-03T18:13:05.752432Z","iopub.status.idle":"2024-09-03T18:13:05.759486Z","shell.execute_reply.started":"2024-09-03T18:13:05.752394Z","shell.execute_reply":"2024-09-03T18:13:05.758274Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check best AUC score on CV\nrf2.best_score_","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T18:13:05.760742Z","iopub.execute_input":"2024-09-03T18:13:05.761071Z","iopub.status.idle":"2024-09-03T18:13:05.776098Z","shell.execute_reply.started":"2024-09-03T18:13:05.761044Z","shell.execute_reply":"2024-09-03T18:13:05.774953Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get all CV scores\nrf2_cv_results = make_results('random forest2 cv', tree2, 'auc')\nrf2_cv_results","metadata":{"execution":{"iopub.status.busy":"2024-09-03T18:13:05.777112Z","iopub.execute_input":"2024-09-03T18:13:05.777434Z","iopub.status.idle":"2024-09-03T18:13:05.797285Z","shell.execute_reply.started":"2024-09-03T18:13:05.777408Z","shell.execute_reply":"2024-09-03T18:13:05.795858Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb2_clf = XGBClassifier(objective='binary:logistic', random_state=0)\n\ncv_params = {'max_depth': [4,6], \n             'min_child_weight': [3,5],\n             'learning_rate': [0.1, 0.2, 0.3],\n             'n_estimators': [5,10,15],\n             #'subsample': [0.7],\n             }    \n\nscoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n\nxgb2 = GridSearchCV(xgb2_clf, cv_params, scoring=scoring, cv=5, refit='roc_auc')","metadata":{"execution":{"iopub.status.busy":"2024-09-03T18:13:05.799087Z","iopub.execute_input":"2024-09-03T18:13:05.799516Z","iopub.status.idle":"2024-09-03T18:13:05.807091Z","shell.execute_reply.started":"2024-09-03T18:13:05.799488Z","shell.execute_reply":"2024-09-03T18:13:05.805668Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nxgb2.fit(X_train, y_train)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T18:13:05.808338Z","iopub.execute_input":"2024-09-03T18:13:05.809412Z","iopub.status.idle":"2024-09-03T18:13:17.059544Z","shell.execute_reply.started":"2024-09-03T18:13:05.809371Z","shell.execute_reply":"2024-09-03T18:13:17.058464Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Best score\nxgb2.best_score_","metadata":{"execution":{"iopub.status.busy":"2024-09-03T18:13:17.061039Z","iopub.execute_input":"2024-09-03T18:13:17.061423Z","iopub.status.idle":"2024-09-03T18:13:17.068901Z","shell.execute_reply.started":"2024-09-03T18:13:17.061391Z","shell.execute_reply":"2024-09-03T18:13:17.067703Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Best parameters\nxgb2.best_params_","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T18:13:17.070493Z","iopub.execute_input":"2024-09-03T18:13:17.07101Z","iopub.status.idle":"2024-09-03T18:13:17.085547Z","shell.execute_reply.started":"2024-09-03T18:13:17.070927Z","shell.execute_reply":"2024-09-03T18:13:17.08442Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb2_cv_results = make_results('xgboost2 cv', xgb2, 'auc')\nxgb2_cv_results","metadata":{"execution":{"iopub.status.busy":"2024-09-03T18:13:17.086932Z","iopub.execute_input":"2024-09-03T18:13:17.087332Z","iopub.status.idle":"2024-09-03T18:13:17.109664Z","shell.execute_reply.started":"2024-09-03T18:13:17.087305Z","shell.execute_reply":"2024-09-03T18:13:17.108453Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_models = pd.concat([tree2_cv_results,rf2_cv_results, xgb2_cv_results], ignore_index=True)\nall_models","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T18:13:17.111096Z","iopub.execute_input":"2024-09-03T18:13:17.111532Z","iopub.status.idle":"2024-09-03T18:13:17.125561Z","shell.execute_reply.started":"2024-09-03T18:13:17.111492Z","shell.execute_reply":"2024-09-03T18:13:17.124346Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get predictions on test data\nxgb2_test_scores = get_scores('xgboost2 test', xgb2, X_test, y_test)\nrf2_test_scores = get_scores('random forest2 test', rf2, X_test, y_test)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T18:13:17.12719Z","iopub.execute_input":"2024-09-03T18:13:17.127638Z","iopub.status.idle":"2024-09-03T18:13:17.248737Z","shell.execute_reply.started":"2024-09-03T18:13:17.1276Z","shell.execute_reply":"2024-09-03T18:13:17.247401Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display the combined test scores\nrf2_vs_xgb2_test = pd.concat([rf2_test_scores, xgb2_test_scores], ignore_index=True)\nrf2_vs_xgb2_test","metadata":{"execution":{"iopub.status.busy":"2024-09-03T18:13:17.250242Z","iopub.execute_input":"2024-09-03T18:13:17.250692Z","iopub.status.idle":"2024-09-03T18:13:17.265136Z","shell.execute_reply.started":"2024-09-03T18:13:17.250652Z","shell.execute_reply":"2024-09-03T18:13:17.263967Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The scores went down by a slight, still random forest performs better when using AUC as deciding metric.","metadata":{}},{"cell_type":"markdown","source":"Let's create confusion matrices to visualize the predictions on test set.","metadata":{}},{"cell_type":"code","source":"y_preds = xgb2.best_estimator_.predict(X_test)\n\n# Array values for confusion matrix\ncm = confusion_matrix(y_test, y_preds, labels=xgb2.classes_)\n\n# Plot CM\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=xgb2.classes_)\ndisp.plot(values_format='', cmap='cividis')\nplt.title('XGboost2 - test set');","metadata":{"execution":{"iopub.status.busy":"2024-09-03T18:13:17.266933Z","iopub.execute_input":"2024-09-03T18:13:17.267408Z","iopub.status.idle":"2024-09-03T18:13:17.564744Z","shell.execute_reply.started":"2024-09-03T18:13:17.267357Z","shell.execute_reply":"2024-09-03T18:13:17.563571Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The XGboost predicted fewer false positives (51) than false negatives (54), meaning it is more likely to miss employees who will quit or get fired, compared to incorrectly identifying employees as quitting or getting fired when they are not. Ideally, to minimize employee attrition, the model should aim for fewer false negatives, even if it results in more false positives, to ensure at-risk employees are identified and appropriate retention measures can be taken. Nevertheless, the model still demonstrates strong predictive capability.\n","metadata":{}},{"cell_type":"code","source":"# Predictions\ny_preds = rf2.best_estimator_.predict(X_test)\n\n# Array values for confusion matrix\ncm = confusion_matrix(y_test, y_preds, labels=rf2.classes_)\n\n# Plot CM\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf2.classes_)\ndisp.plot(values_format='', cmap='cividis')\nplt.title('Random Forest2 - test set');","metadata":{"execution":{"iopub.status.busy":"2024-09-03T18:13:17.566222Z","iopub.execute_input":"2024-09-03T18:13:17.566627Z","iopub.status.idle":"2024-09-03T18:13:17.924568Z","shell.execute_reply.started":"2024-09-03T18:13:17.566593Z","shell.execute_reply":"2024-09-03T18:13:17.92337Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The Random forest predicted more false positives(67) than false negatives(48), that means the model is more likely to incorrectly identify employees as quitting or getting fired (when they actually stay) compared to missing employees who will quit or get fired. \n\nThe goal of the Salifort HR department is to minimize employee attrition because the cost of an employee leaving is very high. Therefore, minimizing false negatives is more important, even at the expense of more false positives. This approach ensures that most employees who are at risk of quitting or getting fired are identified and can be addressed proactively.","metadata":{}},{"cell_type":"markdown","source":"### Conclusion\n\nThe balance between false positives and false negatives reflects the model's trade-offs in predictive capability:\n\nA higher number of false positives indicates more caution in identifying potential attrition but can lead to unnecessary interventions.\nA higher number of false negatives suggests a risk of missing actual cases of attrition, potentially leading to unaddressed employee retention issues.","metadata":{}},{"cell_type":"markdown","source":"### Decision tree splits","metadata":{}},{"cell_type":"markdown","source":"Decision trees provide excellent transparency, making them intuitive and easy to visualize. By examining the splits at each node, one can understand how the model makes decisions, revealing the important features and thresholds it uses for predictions.\n\n\nAdditionally, analyzing these splits helps identify relationships between features and the target variable, highlighting how they interact and influence the model's outcomes. This insight enhances our understanding of the underlying patterns in the data.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(85, 20))\nplot_tree(tree2.best_estimator_, max_depth=6, fontsize=14, feature_names=X.columns,\n         class_names={0:'stayed', 1:'left'}, filled=True)\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T18:13:17.925717Z","iopub.execute_input":"2024-09-03T18:13:17.926043Z","iopub.status.idle":"2024-09-03T18:13:22.123953Z","shell.execute_reply.started":"2024-09-03T18:13:17.926017Z","shell.execute_reply":"2024-09-03T18:13:22.122139Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Random forest feature importance\ntree2_importances = pd.DataFrame(tree2.best_estimator_.feature_importances_, \n                                 columns=['gini_importance'], \n                                 index=X.columns\n                                )\ntree2_importances = tree2_importances.sort_values(by='gini_importance', ascending=False)\n\n# Filtering only features greater than 0\ntree2_importances = tree2_importances[tree2_importances['gini_importance'] != 0]\ntree2_importances","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T18:13:22.125418Z","iopub.execute_input":"2024-09-03T18:13:22.125789Z","iopub.status.idle":"2024-09-03T18:13:22.140294Z","shell.execute_reply.started":"2024-09-03T18:13:22.12576Z","shell.execute_reply":"2024-09-03T18:13:22.139059Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can create barplots to visualize decision tree and random forest feature importances.","metadata":{}},{"cell_type":"code","source":"#DT barplot importance\nsns.barplot(data=tree2_importances, x=\"gini_importance\", y=tree2_importances.index, orient='h',\n           )\nplt.title(\"Decision Tree: Feature Importances for Employee Leaving\", fontsize=12)\nplt.ylabel(\"Feature\")\nplt.xlabel(\"Importance\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T18:13:22.141737Z","iopub.execute_input":"2024-09-03T18:13:22.142088Z","iopub.status.idle":"2024-09-03T18:13:22.426095Z","shell.execute_reply.started":"2024-09-03T18:13:22.142057Z","shell.execute_reply":"2024-09-03T18:13:22.424734Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can observe from the barplot that last_evaluation, number_project, tenure and overworked have the highest importance.\nThese variables are most heplful predictors for the target variable.","metadata":{}},{"cell_type":"markdown","source":"#### Random forest feature importance","metadata":{}},{"cell_type":"code","source":"# Feature importances\nrf_imprt = rf2.best_estimator_.feature_importances_\n\n# Identifies the indices of the 10 most important features of the RF model's best estimator.\n# -10 means that the indices of the 10 largest values are moved to the last 10 positions of the array\n# [-10:] retrieves the indices of these top 10 important features\nind = np.argpartition(rf2.best_estimator_.feature_importances_, -10)[-10:]\n\n# Gets column labels of top 10 features - feat: a list or array containing the feature names\nfeat = X.columns[ind]\n\n# Filter 'feat_impt' to be made of top 10 feature importance scores\nrf_imprt = rf_imprt[ind]\n\n# Creates a DataFrame with 2 columns: \"Feature\" and \"Importance\".\ny_df = pd.DataFrame({\"Feature\":feat,\"Importance\":rf_imprt})\n# Sorts the df by importance \ny_sort_df = y_df.sort_values(\"Importance\")\nfig = plt.figure()\n# The argument 111 means \"1x1 grid, first subplot\", one subplot in the figure.\nax1 = fig.add_subplot(111)\n\n\n# Creates horizontal barplot\ny_sort_df.plot(kind='barh', ax=ax1, x=\"Feature\", y=\"Importance\", color='skyblue')\n\nax1.set_title(\"Random Forest: Feature Importances for Employee Leaving\", fontsize=12)\nax1.set_ylabel(\"Feature\")\nax1.set_xlabel(\"Importance\")\n\nax1.tick_params(axis='x', labelsize=10)\nax1.tick_params(axis='y', labelsize=10)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T18:13:22.42755Z","iopub.execute_input":"2024-09-03T18:13:22.427876Z","iopub.status.idle":"2024-09-03T18:13:22.786348Z","shell.execute_reply.started":"2024-09-03T18:13:22.427848Z","shell.execute_reply":"2024-09-03T18:13:22.785146Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The barplot shows that random forest has the same importance features as decision tree model in the same order as well. These variables are most helpful in predicting the target variable 'left'.","metadata":{}},{"cell_type":"markdown","source":"\nWhen evaluating our machine learning models, our primary focus is on performance and accuracy to ensure accurate predictions on new data, which directly impacts decision-making processes and business outcomes. While consistent feature importance across different models can enhance our understanding and confidence in key features, this is secondary to achieving high predictive performance. The champion model will be selected based on robust metrics such as accuracy, precision, recall, and AUC, ensuring it generalizes well to unseen data.","metadata":{}},{"cell_type":"markdown","source":"# pacE: Execute Stage","metadata":{"id":"401PgchTPr4E"}},{"cell_type":"markdown","source":"\n## Evaluation metrics\n\n- **AUC** is the area under the ROC curve; it's also considered the probability that the model ranks a random positive example more highly than a random negative example.\n- **Precision** measures the proportion of data points predicted as True that are actually True, in other words, the proportion of positive predictions that are true positives.\n- **Recall** measures the proportion of data points that are predicted as True, out of all the data points that are actually True. In other words, it measures the proportion of positives that are correctly classified.\n- **Accuracy** measures the proportion of data points that are correctly classified.\n- **F1-score** is an aggregation of precision and recall.\n\n\n\n\n","metadata":{"id":"ex8pgn5iNzau"}},{"cell_type":"markdown","source":"### Results and Evaluation\n\n\n\n**Logistic Regression:** model achieved precision of 80%, recall of 83%, f1-score of 80% (all weighted averages), and accuracy of 83%, on the test set.\n\n\n\n**Tree-based Machine Learning:** After conducting feature engineering, the decision tree model achieved AUC of 93.8%, precision of 87.0%, recall of 90.4%, f1-score of 88.7%, and accuracy of 96.2%, on the test set. The random forest slightly outperformed the decision tree model.","metadata":{"id":"GXrsxT498Z7h"}},{"cell_type":"markdown","source":"\n### Conclusion and Recommendations\n\nThe models and their extracted feature importances suggest that employees at the company are overworked.\n\nTo improve company's employees retention, the following recommendations could be presented to stakeholders:\n\n* Limit the number of projects assigned to each employee.\n* Consider promoting employees who have been with the company for at least four years, or investigate why these long-tenured employees are so dissatisfied.\n* Either reward employees for working longer hours or eliminate the requirement for them to do so\n* Ensure employees are informed about the company's overtime pay policies. If expectations around workload and time off are unclear, make them explicit.\n* Conduct company-wide and team-specific discussions to understand and improve the company work culture.\n* High evaluation scores should not be reserved for employees working over 200 hours per month. Implement a proportionate scale to reward employees based on their contributions and effort.\n\n### Next Steps\n\nIt may still be reasonable to have concerns about data leakage. It would be prudent to analyze how predictions change when the last_evaluation feature is removed. If evaluations are infrequent, predicting employee retention without this feature could be valuable. Conversely, if evaluation scores significantly influence whether an employee stays or leaves, it might be useful to shift focus and predict performance scores instead. The same consideration applies to the satisfaction score.\n\nWe can try to build a K-means model on this data and analyze clusters as a next project.","metadata":{"id":"9MOMqelNLn2v"}}]}